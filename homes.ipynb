{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77867f4-c9cd-49f9-8ef6-f9724c46fc0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T15:54:11.390540Z",
     "iopub.status.busy": "2023-03-27T15:54:11.390293Z",
     "iopub.status.idle": "2023-03-27T15:54:49.241697Z",
     "shell.execute_reply": "2023-03-27T15:54:49.241186Z",
     "shell.execute_reply.started": "2023-03-27T15:54:11.390514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82a44c079ad4b439d0a2bcf096bb54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>9</td><td>application_1679926457970_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-80-152.ec2.internal:20888/proxy/application_1679926457970_0010/\" class=\"emr-proxy-link\" emr-resource=\"j-2HZAK2JLY9HC9\n",
       "\" application-id=\"application_1679926457970_0010\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-91-99.ec2.internal:8042/node/containerlogs/container_1679926457970_0010_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Filter based on h3 hexagons mapped to polygons\n",
    "\n",
    "aws emr add-steps --cluster-id <Your EMR cluster id> --steps Type=spark,Name=TestJob,Args=[--deploy-mode,cluster,--master,yarn,--conf,spark.yarn.submit.waitAppCompletion=true,s3a://your-source-bucket/code/pythonjob.py,s3a://your-source-bucket/data/data.csv,s3a://your-destination-bucket/test-output/],ActionOnFailure=CONTINUE\n",
    "\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from sedona.register import SedonaRegistrator  \n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from pyspark.sql.functions import udf\n",
    "from sedona.utils.adapter import Adapter\n",
    "#from sedona.core.formatMapper.geojsonReader import GeoJsonReader\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.SpatialRDD import PointRDD, SpatialRDD, CircleRDD\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.enums import GridType\n",
    "from sedona.core.spatialOperator import JoinQueryRaw\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from sedona.core.enums import IndexType\n",
    "from sedona.core.formatMapper.disc_utils import load_spatial_rdd_from_disc, GeoType\n",
    "from sedona.core.formatMapper import WktReader, GeoJsonReader\n",
    "\n",
    "\n",
    "\n",
    "import pyproj\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "from datetime import timedelta, date, datetime\n",
    "from statistics import *\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    "    ArrayType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    from_utc_timestamp,\n",
    "    to_utc_timestamp,\n",
    "    dayofyear,\n",
    "    col,\n",
    "    unix_timestamp,\n",
    "    monotonically_increasing_id,\n",
    "    pandas_udf,\n",
    "    PandasUDFType,\n",
    "    col,\n",
    "    asc,\n",
    "    lit,\n",
    "    countDistinct,\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "from math import *\n",
    "import time\n",
    "\n",
    "from shapely.wkt import loads as wkt_loads\n",
    "from shapely.geometry import Point, Polygon, shape\n",
    "from shapely.ops import transform\n",
    "import shapely\n",
    "\n",
    "#from timezonefinder import TimezoneFinder\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"sedona\")\n",
    "                 .config(\"spark.serializer\", KryoSerializer.getName)          \n",
    "        .config(\"spark.kryo.registrator\",     \n",
    "                  SedonaKryoRegistrator.getName)    \n",
    "         .config(\"spark.driver.maxResultSize\", \"3g\")\n",
    "    .getOrCreate() \n",
    "        )\n",
    "\n",
    "\n",
    "# Register Sedona UDTs and UDFs\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "spark.sparkContext.addPyFile(\"s3://ipsos-dvd/scripts/utils.py\")\n",
    "from utils import *\n",
    "import h3_pyspark as h3s\n",
    "#import h3pandas \n",
    "import h3 as h3\n",
    "\n",
    "\n",
    "\n",
    "@udf(\"boolean\")\n",
    "def pip_filter(poly_wkt, point_x, point_y):\n",
    "    from shapely import wkt\n",
    "    from shapely import geometry\n",
    "    polygon = wkt.loads(poly_wkt)\n",
    "    point = geometry.Point(point_x, point_y)\n",
    "    return polygon.contains(point)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"dirty\", ArrayType(StringType()), False),\n",
    "    StructField(\"hexes\", ArrayType(StringType()), False)\n",
    "])\n",
    "\n",
    "\n",
    "def parse_dates(x):\n",
    "    if \"/\" in x:\n",
    "        start_date = datetime.strptime(x.split('/')[0], \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(x.split('/')[1], \"%Y-%m-%d\")\n",
    "        delta = end_date - start_date\n",
    "        date_list = []\n",
    "        for i in range(delta.days + 1):\n",
    "            date = start_date + timedelta(days = i)\n",
    "            date_list.append(date.strftime(\"%Y/%m/%d\"))\n",
    "        return(date_list)\n",
    "    else: \n",
    "        return([x.replace(\"-\", \"/\")])\n",
    "    \n",
    "def create_point(longitude: float, latitude: float):\n",
    "    return Point(longitude, latitude)\n",
    "\n",
    "create_point_udf = udf(create_point, GeometryType())\n",
    "\n",
    "\n",
    "def create_polygon(wkt: str):\n",
    "    return wkt_loads(wkt)\n",
    "\n",
    "create_polygon_udf = udf(create_polygon, GeometryType())\n",
    "\n",
    "def transform_geometry(geom, crs_from = 'EPSG:4326', crs_to = 'EPSG:9311'):\n",
    "    wgs84 = pyproj.CRS(crs_from)\n",
    "    utm = pyproj.CRS(crs_to)\n",
    "\n",
    "    project = pyproj.Transformer.from_crs(wgs84, utm, always_xy=True).transform\n",
    "\n",
    "    # Ensure that the input geometry is a shapely geometry object\n",
    "    if not isinstance(geom, (shapely.geometry.base.BaseGeometry, shapely.geometry.base.BaseMultipartGeometry)):\n",
    "        geom = shape(geom)\n",
    "\n",
    "    utm_point = transform(project, geom)\n",
    "    \n",
    "    return utm_point\n",
    "\n",
    "transform_geometry_udf = udf(transform_geometry, GeometryType())\n",
    "\n",
    "\n",
    "def shared_polygon(long, lat):\n",
    "    return Point(long, lat).buffer(35)\n",
    "shared_polygon_udf = udf(shared_polygon, GeometryType())\n",
    "\n",
    "def buffer(geom, meters):\n",
    "    return geom.buffer(meters)\n",
    "buffer_udf = udf(buffer, GeometryType())\n",
    "\n",
    "def parse_dates(x):\n",
    "    if \"/\" in x:\n",
    "        start_date = datetime.strptime(x.split('/')[0], \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(x.split('/')[1], \"%Y-%m-%d\")\n",
    "        delta = end_date - start_date\n",
    "        date_list = []\n",
    "        for i in range(delta.days + 1):\n",
    "            date = start_date + timedelta(days = i)\n",
    "            date_list.append(date.strftime(\"%Y/%m/%d\"))\n",
    "        return(date_list)\n",
    "    else: \n",
    "        return([x.replace(\"-\", \"/\")])\n",
    "\n",
    "\n",
    "def time_zone(long, lat):\n",
    "    #return tzwhere.tzwhere().tzNameAt(lat, long)\n",
    "    tzf = TimezoneFinder()\n",
    "    return tzf.timezone_at(lng=long, lat=lat)\n",
    "\n",
    "time_zone_udf = udf(time_zone, StringType())\n",
    "\n",
    "#------\n",
    "# parameters\n",
    "#------\n",
    "\n",
    "data_dir = \"s3://external-safegraph/\"\n",
    "data_dyn = \"s3://ipsos-dvd/dyn/data/\"\n",
    "data_veraset = \"s3://external-veraset-data-us-west-2/us/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0455281-da52-4292-bf52-84dfe9c30194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date = \"2022/06/01\"\n",
    "path = data_veraset + date\n",
    "redo_points = False\n",
    "\n",
    "if redo_points:\n",
    "    essential_fields = [\n",
    "            StructField(\"utc_timestamp\",LongType(),False),\n",
    "            StructField(\"caid\",StringType(),False),\n",
    "            StructField(\"latitude\",DoubleType(),False),\n",
    "            StructField(\"longitude\",DoubleType(),False),\n",
    "            StructField(\"altitude\",DoubleType(),False),\n",
    "    ]\n",
    "    raw_schema = StructType(\n",
    "        essential_fields + [\n",
    "            StructField(\"id_type\",StringType(),False),\n",
    "            StructField(\"geo_hash\",StringType(),False),\n",
    "            StructField(\"horizontal_accuracy\",DoubleType(),False),\n",
    "            StructField(\"ip_address\",StringType(),False),\n",
    "            #StructField(\"altitude\",DoubleType(),False),\n",
    "            StructField(\"iso_country_code\",StringType(),False)]\n",
    "    )\n",
    "    pings = spark.read.schema(raw_schema).parquet(path).select(\"latitude\", \"longitude\", \"caid\", \"utc_timestamp\")# .limit(1000)\n",
    "    pings.createOrReplaceTempView(\"pings\")\n",
    "\n",
    "    # Read Hive table\n",
    "    pings = spark.sql(\n",
    "          \"\"\"SELECT ST_Point(cast(pings.longitude as Decimal(24,20)), cast(pings.latitude as Decimal(24,20))) AS point, \n",
    "          utc_timestamp, caid\n",
    "          FROM pings;\n",
    "          \"\"\"\n",
    "    )\n",
    "    pings.write.mode(\"overwrite\").parquet(data_dyn + \"pings/\" + date)\n",
    "\n",
    "\n",
    "pings = spark.read.parquet(data_dyn + \"pings/\" + date) #.limit(1000000)\n",
    "pings = pings.repartition(10000, \"caid\")\n",
    "\n",
    "\n",
    "#                 .withColumn(\"date\", # filter on nighttime hours\n",
    "#                     from_utc_timestamp(\n",
    "#                         col(\"utc_timestamp\").cast(dataType=TimestampType()),\n",
    "#                         col('UTC_OFFSET'),\n",
    "#                     )\n",
    "#                 )\n",
    "#                 .withColumn('hour', F.hour(col('date')))\n",
    "#                 .filter((col('hour').between(20,24)) | (col('hour').between(0,7))) # between 8pm and 7am\n",
    "#                 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3c5792-3fda-4ffe-a7de-188554bec1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T15:57:56.261621Z",
     "iopub.status.busy": "2023-03-27T15:57:56.261372Z",
     "iopub.status.idle": "2023-03-27T15:57:58.527625Z",
     "shell.execute_reply": "2023-03-27T15:57:58.527105Z",
     "shell.execute_reply.started": "2023-03-27T15:57:56.261595Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c81665de7f743169c08e6662de95e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn = \"s3://ipsos-dvd/data/Time_Zones/\"\n",
    "\n",
    "# timezone shapefile to hive\n",
    "test = gpd.read_file(\"s3://ipsos-dvd/data/Time_Zones/\")\n",
    "test = test.to_crs(\"epsg:4326\")[['utc', 'geometry']]\n",
    "shpf = spark.createDataFrame(test)\n",
    "shpf = shpf.withColumn(\"utc\", F.concat(lit(\"UTC\"), F.col(\"utc\")))\n",
    "shpf = shpf.cache()\n",
    "\n",
    "# cbg shapefile to hive\n",
    "fn = \"s3://ipsos-dvd/ev/data/2020_cbgs/\"\n",
    "polygons_rdd = ShapefileReader.readToGeometryRDD(sc, fn)\n",
    "polygons_rdd = (Adapter.toDf(polygons_rdd, polygons_rdd.fieldNames, spark))\n",
    "\n",
    "polygons_rdd = polygons_rdd.repartition(5000)\n",
    "\n",
    "# filter out zero population CBGs\n",
    "pop = spark.read.csv(os.path.join(data_dyn, 'census', 'census_wrangle.csv.gz'), header = True).select(\"census_block_group\", \"pop\")\n",
    "polygons_rdd = polygons_rdd.join(pop, pop.census_block_group == polygons_rdd.CensusBloc, how = \"left\")\n",
    "polygons_rdd = polygons_rdd.filter(F.col(\"pop\") > 0)\n",
    "polygons_rdd = polygons_rdd.drop(\"census_block_group\", \"pop\")#.createOrReplaceTempView(\"polygons_rdd\")\n",
    "polygons_rdd = polygons_rdd.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68facf52-0723-4151-bca4-3a31e39dbc70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T16:15:39.938587Z",
     "iopub.status.busy": "2023-03-27T16:15:39.938340Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65c2320b70643b3ba17f061448a9790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0be1249115f4fe7893d0b8b8fc00f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use spatial partitioning\n",
    "grid_type = GridType.KDBTREE\n",
    "\n",
    "\n",
    "START_DATE = \"2022-06-02\"\n",
    "END_DATE = \"2022-06-30\"\n",
    "dates = [START_DATE + \"/\" + END_DATE]\n",
    "\n",
    "# set up date ranges in path form\n",
    "datelist = []\n",
    "\n",
    "for arg in dates:\n",
    "    temp = parse_dates(arg)\n",
    "    datelist.extend(temp)\n",
    "# create list of s3 folders for dates to get\n",
    "base_bucket = f\"s3://external-veraset-data-us-west-2/us/\" # f\"s3://external-veraset-data-us-west-2/us/\" # f\"s3://external-veraset-data-us-west-2/movement/\"\n",
    "#base_bucket = f\"/home/antonvocalis/ipsos/data/in/veraset/\"\n",
    "\n",
    "for date in datelist:\n",
    "    path = data_veraset + date\n",
    "    redo_points = False\n",
    "\n",
    "    if redo_points:\n",
    "        essential_fields = [\n",
    "                StructField(\"utc_timestamp\",LongType(),False),\n",
    "                StructField(\"caid\",StringType(),False),\n",
    "                StructField(\"latitude\",DoubleType(),False),\n",
    "                StructField(\"longitude\",DoubleType(),False),\n",
    "                StructField(\"altitude\",DoubleType(),False),\n",
    "        ]\n",
    "        raw_schema = StructType(\n",
    "            essential_fields + [\n",
    "                StructField(\"id_type\",StringType(),False),\n",
    "                StructField(\"geo_hash\",StringType(),False),\n",
    "                StructField(\"horizontal_accuracy\",DoubleType(),False),\n",
    "                StructField(\"ip_address\",StringType(),False),\n",
    "                #StructField(\"altitude\",DoubleType(),False),\n",
    "                StructField(\"iso_country_code\",StringType(),False)]\n",
    "        )\n",
    "        pings = spark.read.schema(raw_schema).parquet(path).select(\"latitude\", \"longitude\", \"caid\", \"utc_timestamp\")# .limit(1000)\n",
    "        pings.createOrReplaceTempView(\"pings\")\n",
    "\n",
    "        # Read Hive table\n",
    "        pings = spark.sql(\n",
    "              \"\"\"SELECT ST_Point(cast(pings.longitude as Decimal(24,20)), cast(pings.latitude as Decimal(24,20))) AS point, \n",
    "              utc_timestamp, caid\n",
    "              FROM pings;\n",
    "              \"\"\"\n",
    "        )\n",
    "        pings.write.mode(\"overwrite\").parquet(data_dyn + \"pings/\" + date)\n",
    "\n",
    "\n",
    "    pings = spark.read.parquet(data_dyn + \"pings/\" + date).sample(fraction = 0.25)  #.limit(1000000)\n",
    "    \n",
    "#     # Define the number of salt keys and the number of output partitions\n",
    "#     num_salt_keys = 1000\n",
    "    num_partitions = 10000\n",
    "\n",
    "#     # Choose the original column to repartition on\n",
    "#     repartition_column = \"caid\"\n",
    "\n",
    "#     # Create a new salted repartition column by concatenating the original column with a random salt key\n",
    "#     pings = pings.withColumn(\"salted_repartition_column\", F.concat(F.col(repartition_column), (F.rand() * num_salt_keys).cast(\"int\")))\n",
    "\n",
    "    # Repartition the data using the salted repartition column\n",
    "    pings = pings.repartition(num_partitions, \"caid\")\n",
    "\n",
    "#     # broadcast join\n",
    "#     pings = pings.alias(\"pings\").join(F.broadcast(shpf).alias(\"shpf\"), F.expr(\n",
    "#       f\"\"\"ST_Within(pings.point, shpf.geometry) \"\"\"\n",
    "#     )).select(\"pings.utc_timestamp\", \"pings.caid\", \"pings.point\", \"shpf.utc\")\n",
    "\n",
    "\n",
    "#     pings = pings.cache()\n",
    "\n",
    "#     #     pings.createOrReplaceTempView(\"pings\")\n",
    "\n",
    "#     #     # Read Hive table\n",
    "#     #     pings = spark.sql(\n",
    "#     #           \"\"\"SELECT latitude, longitude, caid, utc_timestamp, point, UTC_OFFSET\n",
    "#     #             FROM pings, shpf\n",
    "#     #             WHERE ST_Within(pings.point, shpf.geometry)\n",
    "#     #           \"\"\"\n",
    "#     #     )\n",
    "#     pings = (pings.withColumn(\"date\", # filter on nighttime hours\n",
    "#         from_utc_timestamp(\n",
    "#             col(\"utc_timestamp\").cast(dataType=TimestampType()),\n",
    "#             col('utc'),\n",
    "#         )\n",
    "#     )\n",
    "#     .withColumn('hour', F.hour(col('date')))\n",
    "#     .filter((col('hour') >= 20) | (col('hour') < 7))\n",
    "#     .drop(\"utc\", \"date\", \"hour\")\n",
    "#     )\n",
    "#     pings = pings.cache()\n",
    "    \n",
    "    #pings.createOrReplaceTempView(\"pings\")\n",
    "\n",
    "#     pings = pings.alias(\"pings\").join(F.broadcast(polygons_rdd).alias(\"polygons_rdd\"), F.expr(\n",
    "#       f\"\"\"ST_Within(pings.point, polygons_rdd.geometry)\"\"\"\n",
    "#     )).select(\"pings.utc_timestamp\", \"pings.caid\", \"polygons_rdd.CensusBloc\")\n",
    "    \n",
    "    \n",
    "\n",
    "#     pings = spark.sql(\n",
    "#         \"\"\"\n",
    "#         SELECT caid, utc_timestamp, point, CensusBloc\n",
    "#             FROM pings, polygons_rdd\n",
    "#             WHERE ST_Within(pings.point, polygons_rdd.geometry)\n",
    "#         \"\"\"\n",
    "#     )\n",
    "#     pings.write.mode(\"overwrite\").parquet(os.path.join(data_dyn, 'pings_homes', date))\n",
    "\n",
    "    \n",
    "    #pings = pings.cache()\n",
    "    # convert to spatial rdds\n",
    "    points_rdd = Adapter.toSpatialRdd(pings, \"point\")#.transform('epsg:4326', 'epsg:9311')\n",
    "    \n",
    "\n",
    "    points_rdd.analyze()\n",
    "    points_rdd.spatialPartitioning(grid_type)\n",
    "\n",
    "#     fn = \"s3://ipsos-dvd/ev/data/2020_cbgs/\"\n",
    "#     polygons_rdd = ShapefileReader.readToGeometryRDD(sc, fn)\n",
    "    poly_rdd = Adapter.toSpatialRdd(polygons_rdd, \"geometry\")#.transform('epsg:4326', 'epsg:9311')\n",
    "\n",
    "\n",
    "    poly_rdd.analyze()\n",
    "    poly_rdd.spatialPartitioning(points_rdd.getPartitioner())\n",
    "\n",
    "    build_on_spatial_partitioned_rdd = True ## Set to TRUE only if run join query\n",
    "    using_index = True\n",
    "    points_rdd.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n",
    "    \n",
    "    # spatial join\n",
    "    result = JoinQueryRaw.SpatialJoinQueryFlat(points_rdd,poly_rdd, using_index, True)\n",
    "\n",
    "    # (result.indexedRawRDD.saveAsObjectFile(\"hdfs://PATH\") #saveAsTextFile(data_dyn + \"temp/spatial_join_test\")\n",
    "    # )\n",
    "    # (result.write\n",
    "    #   .option(\"header\", \"true\")\n",
    "    #   .csv(data_dyn + \"spatial_join_test\")\n",
    "    # )\n",
    "    (Adapter.toDf(result, poly_rdd.fieldNames, points_rdd.fieldNames, spark)#.show(5)\n",
    "        .write.mode(\"overwrite\").parquet(os.path.join(data_dyn, 'pings_homes', date))\n",
    "    ) # this is currently slow, may need to remove unrealistically large polygons?\n",
    "    \n",
    "    pings.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84869e-fca9-4b15-b178-bfa86c12ded1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
